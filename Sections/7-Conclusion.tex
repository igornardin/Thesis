\chapter{Conclusion and Perspectives}
\label{cha:conclusion}

In this chapter, we make overall conclusions and summarize the contributions of this work, along with a discussion of future work.

\section{Conclusion}

The world has a big challenge to reduce its GHG emissions and stop global warming. GHG reduction demands the engagement of all sectors of governments, industry, and research. One sector is Information and Communication Technology (ICT) with emissions in 2020 around 1.8\%-2.8\% or 2.1\%-3.9\% considering the supply chain pathways. Data centers are one of the actors inside ICT with a good share of the emissions. Both academy and industries focus their efforts to reduce the emissions of the big data center providers. Reducing their emissions is particularly difficult due to the growth in Internet users and networked devices. Even with this growth, the emissions did not increase at the same pace due mainly to the power optimizations. However, some authors claim that these power consumption improvements are close to their limit. Therefore, data center providers must find other ways to reduce the emissions, such as renewable energy migration.

Some providers, such as Google and Amazon, are investing in an off-site renewable production approach to maintain their servers. In an off-site generation, the data center and power production are not in the same place. So, the providers generate the same amount of energy to the grid that they expend on their servers. However, the biggest challenge of renewable sources is the power intermittence. Since RES production comes from nature, it depends on the climate conditions. Therefore, these providers transfer the problem to third parties. In a net zero scenario (where no energy comes from brown sources), they can not maintain their quality of service without changing the strategy. 

The Datazero2 project proposes an architecture and decision process for a renewable-only data center. The architecture is disconnected from the power grid, using only the energy produced by solar panels and wind turbines but with power storage as backups. Using just renewable sources demand different decision levels. We divided the decision level into two possibilities: offline and online. The offline approach has time to search for an optimal solution to match power production and demand. Usually, it predicts both production and demand. The main drawback of the offline approach is that it does not know the real events, so its optimal solution only works if the predictions are good enough. On the other hand, the online approach reacts to real events. However, an online-only approach is myopic, ignoring long-term decisions. In this thesis, we proposed ways to mix offline and online. We demonstrated in Chapter \ref{cha:related_work} a lack of propositions mixing both. The main objectives of an integrated offline-online approach are to improve the QoS (maximizing finished jobs and reducing killed jobs), respect the power constraints (approximating battery target level), and reduce the wasted energy.

First, we focused on respecting the power constraints. So, we proposed four compensation policies to adapt power usage, approximating the battery target level at the end of the time window. The compensation changes the power usage according to the power fluctuations derived from production/consumption variations. The compensation policies are \emph{Peak}, \emph{Last}, \emph{Next}, and \emph{Workload}. All policies arrive with better battery levels than the baselines (\emph{Follow plan}, \emph{Power reactive}, and \emph{Workload reactive}). The policies improved the finished jobs in the scenarios with more energy available, compared to an offline-only execution (\emph{Follow plan}). In scenarios with less energy available, they reduced the number of finished jobs but approximated the target battery level at the end of the time window. Nevertheless, there was not a good policy in every execution, demanding further improvements.

After, we tried to introduce a Reinforcement Learning (RL) model to make better compensation decisions. Since every policy arrived at a good battery level at the end of the time window, we tried to mix the compensations driven by a QoS metric. Therefore, we proposed an RL model with two reward propositions linked to QoS (finished, killed, and started jobs). The idea is still to compensate for power variations, which proved to be enough to respect the power constraint, but choosing the actions which improved QoS. We proposed two RL algorithms: Q-Learning and Contextual Multi-Armed Bandit with LinUCB. The results show that they could not improve the Qos, resulting in even worse QoS. These results were explained by several factors, such as chained decisions making it harder to find the best actions, the reward did not represent the global QoS, state/action size limitations, and separated scheduling and power decisions.

Finally, the last contribution is a heuristic which consolidates all the aspects discovered in previous sections. The heuristic is named \emph{\systemName} and englobes the compensation idea (from the policies) and solves the problems from the Reinforcement Learning, such as mixing scheduling and power decisions and using more information from the state of charge estimation (impossible in RL due state/action size limitations). We compared \emph{\systemName} with the baselines and policies, showing good overall results of the new heuristic. \emph{\systemName} had the lowest killed jobs than all the other algorithms while respecting the power constraints. Regarding the finished jobs, it was among the top-3 in every scenario. \emph{Workload reactive} maintains the servers sleeping until a job arrives. This behavior helps this algorithm to have low wasted energy. Even so, \emph{\systemName} had lower wasted energy than \emph{Workload reactive} in three of five cases. 

\section{Perspectives}

The work presented in this thesis proposes some ways to mix offline and online decisions in a renewable-only data center. As perspectives, there are some remaining subjects for further improvement.

\textbf{Change learning process}
% Other RL algorithms
% Deep RL

As we presented in Chapter \ref{cha:learning_power_compensations}, our model for learning the best compensations did not improve the policy's results. We presented several problems in our model. Therefore, future work can introduce a different learning algorithm, such as Deep Reinforcement Learning (DRL). DRL allows a larger state and action space. Using a larger state space, we could introduce more variables, such as jobs waiting to run, estimated SoC of the future steps (time series), the actual state of running jobs, etc. Besides, the action space can be more precise, using the right step to compensate. Future work can compare DRL with the heuristics in the time and energy spent to learn and apply the algorithms. Another possibility is changing our reward for a more global metric, such as the time window finished and killed jobs. This reward demands more learning iterations in a simple RL but could produce better results. We hypothesized that doing the best local actions would result in the best global results. However, we saw that this is not true. 

\textbf{Add flexibility in energy constraints}
% Add a \% of flexibility in the target level
% Use hydrogen

The model presented in Chapter \ref{cha:model} considered a hard constraint of the battery's SoC at the end of the time window. Even if the algorithms (policies and \emph{\systemName}) did not finish with the exact target level, their decisions consider this battery constraint. Future work can evaluate the possibility of a flexible battery level. For example, they can accept $\pm 5\%$ as flexibility at the end of the time window. This flexibility can improve even more the QoS, running more jobs and reducing killed jobs. However, this flexibility must be studied along with the PDM module. Giving $\pm 5\%$ as the battery's flexibility would finish the battery with $-5\%$ than the target in every scenario (the algorithms use all the possible energy available). Nevertheless, this behavior can lead to PDM demanding an overestimated target level (e.g., always $+5\%$), which does not change the actual behavior. Another possibility is to introduce hydrogen decisions in the model. In the current model, we considered hydrogen production as fixed input production without changes due to the hydrogen warming-up time. However, the online model can introduce this warming-up time and use hydrogen in critical cases. As for the battery's flexibility, this must be studied together with PDM.

\textbf{Modify the application type}
% Services in general
% Streaming

This thesis focused on HPC batch applications. Future work can evaluate \emph{\systemName} and the policies for other application types. For example, let's imagine a renewable-only streaming scheduler using \emph{\systemName}. A new user sends a request to watch a video from the catalog. The catalog has several videos with different sizes. The video size can be our walltime, which \emph{\systemName} uses to estimate the energy demanded. The user should wait for his time to watch (supposing that they agreed with this to use clean energy). Changing the processor's speed would impact the video quality. Killing a job interrupts the streaming. In dangerous moments (SoC close to the battery lower boundary), \emph{\systemName} lets only small videos. We can evaluate the QoS (waiting time, quality degradation, etc) of \emph{\systemName} in this environment. A possible uncertainty comes from the possibility of pausing the video, which changes the video's finish time. Another possible future work is to introduce in the model other job aspects, such as memory, network communication, etc. This thesis considered that the jobs are impacted only by the CPU. Even if this is the main factor, it is not the only one.

\textbf{Explore different time definitions}
% time window
% time step
% chained time windows

Our work considered a time window of three days and a time step of five minutes. The time window size is not too long to make it difficult to predict the weather conditions and not too short to reduce ODM decision possibilities. In addition, the time step is short enough to maintain ODM reactive (e.g., fastly adapt battery usage) and long enough to finish the server transitions (e.g., the server on and off). However, new works can compare different time windows and time steps, finding the best configurations. For example, would a time window of one day impact the ODM decisions? And a time window of one week? Does a longer time step be too late to adapt power usage? Another possible evaluation is to evaluate chained time windows. For example, the execution of Datazero2 during one year with all modules integrate. This thesis created some offline modules without a direct connection with the online. We ran the offline optimizations, generating input for the online experiments. A complete one-year execution demands the total Datazero2 middleware implementation. 